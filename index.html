<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Scott Pesme</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Scott Pesme</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="about_me.html">About&nbsp;me</a></div>
<div class="menu-item"><a href="enigmas.html">Enigmas</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Scott Pesme</h1>
</div>
<table class="imgtable"><tr><td>
<img src="Photo/Scott_Lyngen.jpg" alt="" width="200px" height="241px" />&nbsp;</td>
<td align="left"><h2>Who am I</h2>
<p>I am second year PhD student at EPFL in the Theory of Machine Learning group under the supervision of <a href="https://www.di.ens.fr/~flammarion/">Nicolas Flammarion</a>.</p>
<p>I graduated from <a href="https://www.polytechnique.edu/">École Polytechnique</a> in 2019 and have a master degree from <a href="https://ens-paris-saclay.fr/en/">ENS Paris-Saclay</a> in mathematics, vision and machine learning (<a href="https://www.master-mva.com/">MVA</a>).
I did my master thesis in convex optimisation at EPFL in the MLO group under the supervision of <a href="http://www.cmap.polytechnique.fr/~aymeric.dieuleveut/">Aymeric Dieuleveut</a> and <a href="https://people.epfl.ch/martin.jaggi">Martin Jaggi</a>.</p>
<h2>Contact</h2>
<ul>
<li><p><b>Physical address</b>: Ecole Polytechnique Fédérale de Lausanne, 3rd floor, <a href="mailto:https://www.google.com/maps/place/INJ,+1015+Ecublens/@46.518233,6.5635998,19.12z/data=!4m8!1m2!2m1!1sEPFL+lausanne!3m4!1s0x478c30fcf8d67b97:0x66ba7fbc591afbf0!8m2!3d46.5183589!4d6.5637812">Route J-D. Colladon, 1015 Lausanne</a>.</p>
</li>
<li><p><b>Virtual address</b>: scott [dot] pesme [at] epfl [dot] ch</p>
</li>
</ul>
</td></tr></table>
<h2>Research interests</h2>
<p>My main research interests are at the intersection between optimisation and statistics. Here is a selection of research topics I am interested in:</p>
<ul>
<li><p>Optimisation methods in machine learning (convex and more recently non-convex)</p>
</li>
<li><p>Stochastic Differential Equations and how they can help understand optimisation algorithms </p>
</li>
</ul>
<h2>Publications</h2>
<p><p style="margin-bottom:1cm;"&gt; </p></p>
<ul>
<li><p>S. Pesme, L. Pillaud-Vivien, N. Flammarion. <b>Implicit Bias of SGD for Diagonal Linear Networks: a Provable Benefit of Stochasticity</b>. 
<br /> [<a href="https://arxiv.org/abs/2106.09524">arxiv</a>, <a href="Articles/implicit_bias_SGD_DNN.pdf">pdf</a>], <i>Preprint</i>, 2021. <span class="toggle-trigger">[Show Abstract]<span> 
<div class="toggle-wrap">
<b>Abstract:</b> Understanding the implicit bias of training algorithms is of crucial importance in order to explain the success of overparametrised neural networks. In this paper, we study the dynamics of stochastic gradient descent over diagonal linear networks through its continuous time version, namely stochastic gradient flow. We explicitly characterise the solution chosen by the stochastic flow and prove that it always enjoys better generalisation properties than that of gradient flow. Quite surprisingly, we show that the convergence speed of the training loss controls the magnitude of the biasing effect: the slower the convergence, the better the bias. To fully complete our analysis, we provide convergence guarantees for the dynamics. We also give experimental results which support our theoretical claims. Our findings highlight the fact that structured noise can induce better generalisation and they help explain the greater performances observed in practice of stochastic gradient descent over gradient descent.
</div></p>
</li>
</ul>
<ul>
<li><p>S. Pesme, N. Flammarion. <b>Online Robust Regression via SGD on the l1 loss</b>. 
<br /> [<a href="https://arxiv.org/abs/2007.00399">arxiv</a>, <a href="Articles/Neurips_Camera_ready.pdf">pdf</a>], <i>NeurIPS</i>, 2020. </p>
</li>
</ul>
<ul>
<li><p>S. Pesme, A. Dieuleveut, N. Flammarion.<br /><b>On Convergence-Diagnostic based Step Sizes for Stochastic Gradient Descent</b>. 
<br /> [<a href="https://arxiv.org/abs/2007.00534">arxiv</a>, <a href="Articles/ICML_camera_ready.pdf">pdf</a>], <i>ICML</i>, 2020. <span class="toggle-trigger">[Show Abstract]<span> 
<div class="toggle-wrap">
<b>Abstract:</b> Constant step-size Stochastic Gradient Descent exhibits two phases: a transient phase during which iterates make fast progress towards the optimum, followed by a stationary phase during which iterates oscillate around the optimal point. In this paper, we show that efficiently detecting this transition and appropriately decreasing the step size can lead to fast convergence rates. We analyse the classical statistical test proposed by Pflug (1983), based on the inner product between consecutive stochastic gradients. Even in the simple case where the objective function is quadratic we show that this test cannot lead to an adequate convergence diagnostic. We then propose a novel and simple statistical procedure that accurately detects stationarity and we provide experimental results showing state-of-the-art performance on synthetic and real-world datasets. 
</div></p>
</li>
</ul>
<p>You can have a look at my <a href="https://scholar.google.com/citations?user=BwCLRb0AAAAJ&amp;hl=en">Google scholar</a> webpage, though you will not find any additional paper.</p>
<div id="footer">
<div id="footer-text">
Page generated 2021-07-06 08:30:00 CEST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script>
$(document).ready(function() {
    $(".toggle-trigger").click(function() {
        $(this).parent().nextAll('.toggle-wrap').first().slideToggle('slow');
    });
});
</script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-121611765-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-121611765-1');
</script>
</body>
</html>
